{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"_kg_hide-input":false,"id":"VzOC-SwEQRPg","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1589507136912,"user_tz":240,"elapsed":14414,"user":{"displayName":"Hariharan Ramasubramanian","photoUrl":"https://lh4.googleusercontent.com/-8PbViz9yTsI/AAAAAAAAAAI/AAAAAAAAAA8/GblK5L_Sbnw/s64/photo.jpg","userId":"09657672106810852044"}},"outputId":"cc48dfd2-96f6-4ec0-dd85-9da8f0bf5841"},"source":["import os\n","os.system('pip install pytorch_toolbelt')\n","import pandas as pd\n","import numpy as np\n","import json\n","pd.options.display.max_rows = 1000\n","pd.options.display.max_columns = 1000\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score\n","import time\n","\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from functools import partial\n","from sklearn.model_selection import KFold\n","import gc\n","\n","from tqdm import tqdm\n","from itertools import groupby, accumulate\n","from random import shuffle\n","\n","from sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\n","from sklearn.preprocessing import MinMaxScaler\n","from pytorch_toolbelt import losses as L\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"unaUKmCoUpI4","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1589507172556,"user_tz":240,"elapsed":29509,"user":{"displayName":"Hariharan Ramasubramanian","photoUrl":"https://lh4.googleusercontent.com/-8PbViz9yTsI/AAAAAAAAAAI/AAAAAAAAAA8/GblK5L_Sbnw/s64/photo.jpg","userId":"09657672106810852044"}},"outputId":"53dcc616-6aa0-43d5-d53f-adb3fe47c900"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3VwEuJF1VD0N","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1589507177879,"user_tz":240,"elapsed":651,"user":{"displayName":"Hariharan Ramasubramanian","photoUrl":"https://lh4.googleusercontent.com/-8PbViz9yTsI/AAAAAAAAAAI/AAAAAAAAAA8/GblK5L_Sbnw/s64/photo.jpg","userId":"09657672106810852044"}},"outputId":"f17f5841-0389-4372-b473-2ac3dbfa5360"},"source":["cd drive/My Drive/KaggleChallenges/Liverpool #change here"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/My Drive/KaggleChallenges/Liverpool #change here'\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0moANYjbUhqC"},"source":["ss = pd.read_csv(\"./train/sample_submission.csv\", dtype={'time':str})\n","train = pd.read_csv('./train/train_clean.csv')\n","train['filter'] = 0\n","test = pd.read_csv('./train/test_clean.csv')\n","test['filter'] = 2\n","ts1 = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)\n","\n","ts1['time2'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14 + 1), labels=list(range(14)), include_lowest=True).astype(int)\n","ts1['time2'] = ts1.groupby('time2')['time'].rank( )/500000.\n","\n","np.random.seed(321)\n","ts1['group'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14*125 + 1), labels=list(range(14*125)), include_lowest=True).astype(int)\n","np.random.seed(321)\n","\n","y = ts1.loc[ts1['filter']==0, 'open_channels']\n","group = ts1.loc[ts1['filter']==0, 'group']\n","X = ts1.loc[ts1['filter']==0, 'signal']\n","\n","np.random.seed(321)\n","skf = GroupKFold(n_splits=5)\n","splits = [x for x in skf.split(X, y, group)]\n","\n","use_cols = [col for col in ts1.columns if col not in ['index','filter','group', 'open_channels', 'time', 'time2']]\n","\n","# Create numpy array of inputs\n","for col in use_cols:\n","    col_mean = ts1[col].mean()\n","    ts1[col] = ts1[col].fillna(col_mean)\n","\n","val_preds_all = np.zeros((ts1[ts1['filter']==0].shape[0], 11))\n","test_preds_all = np.zeros((ts1[ts1['filter']==2].shape[0], 11))\n","\n","groups = ts1.loc[ts1['filter']==0, 'group']\n","times = ts1.loc[ts1['filter']==0, 'time']\n","\n","new_splits = []\n","for sp in splits:\n","    new_split = []\n","    new_split.append(np.unique(groups[sp[0]]))\n","    new_split.append(np.unique(groups[sp[1]]))\n","    new_splits.append(new_split)\n","\n","trainval = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)))\n","test = np.array(list(ts1[ts1['filter']==2].groupby('group').apply(lambda x: x[use_cols].values)))\n","trainval_y = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[['open_channels']].values)))\n","\n","gc.collect()\n","# transpose to B x C x L\n","trainval = trainval.transpose((0,2,1))\n","test = test.transpose((0,2,1))\n","\n","trainval_y = trainval_y.reshape(trainval_y.shape[:2])\n","test_y = np.zeros((test.shape[0], trainval_y.shape[1]))\n","\n","trainval = torch.Tensor(trainval)\n","test = torch.Tensor(test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdL6ajdMS0NN"},"source":["class EarlyStopping:\n","    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n","        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","        self.counter, self.best_score = 0, None\n","        self.is_maximize = is_maximize\n","\n","    def load_best_weights(self, model):\n","        model.load_state_dict(torch.load(self.checkpoint_path))\n","\n","    def __call__(self, score, model):\n","        if self.best_score is None or \\\n","        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","            torch.save(model.state_dict(), self.checkpoint_path)\n","            self.best_score, self.counter = score, 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0QnnfpdYF_7"},"source":["class Seq2SeqRnn(nn.Module):\n","    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n","            hidden_layers = [100, 200]):\n","\n","        super().__init__()\n","        self.input_size = input_size\n","        self.seq_len = seq_len\n","        self.hidden_size = hidden_size\n","        self.num_layers=num_layers\n","        self.bidirectional=bidirectional\n","        self.output_size=output_size\n","\n","        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n","                           bidirectional=bidirectional, batch_first=True,dropout=0.3)\n","         # Input Layer\n","        if hidden_layers and len(hidden_layers):\n","            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n","\n","            # Hidden Layers\n","            self.hidden_layers = nn.ModuleList(\n","                [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n","            )\n","            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)\n","\n","            self.intermediate_layer = nn.Linear(hidden_layers[-1], self.input_size)\n","            # output layers\n","            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n","            nn.init.kaiming_normal_(self.output_layer.weight.data)\n","\n","        else:\n","            self.hidden_layers = []\n","            self.intermediate_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_siz, self.input_size)\n","            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n","            nn.init.kaiming_normal_(self.output_layer.weight.data)\n","\n","        self.activation_fn = torch.relu\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = x.permute(0,2,1)\n","\n","        outputs, hidden = self.rnn(x)\n","\n","        x = self.dropout(self.activation_fn(outputs))\n","        for hidden_layer in self.hidden_layers:\n","            x = self.activation_fn(hidden_layer(x))\n","            x = self.dropout(x)\n","\n","        x = self.output_layer(x)\n","\n","        return x\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EM7v3DRgYRFg"},"source":["class IonDataset(Dataset):\n","    \"\"\"Car dataset.\"\"\"\n","\n","    def __init__(self, data, labels, training=True, transform=None, flip=0.5, noise_level=0, class_split=0.0):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","        self.training = training\n","        self.flip = flip\n","        self.noise_level = noise_level\n","        self.class_split = class_split\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        data = self.data[idx]\n","        labels = self.labels[idx]\n","        if np.random.rand() < self.class_split:\n","            data, labels = class_split(data, labels)\n","        if  np.random.rand() < self.noise_level:\n","            data = data * torch.FloatTensor(10000).uniform_(1-self.noise_level, 1+self.noise_level)\n","        if np.random.rand() < self.flip:\n","            data = torch.flip(data, dims=[1])\n","            labels = np.flip(labels, axis=0).copy().astype(int)\n","\n","        return [data, labels.astype(int)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5IwlJ5RTE_n","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ee7b8850-683f-4494-f5e2-f4bcaf9d4467"},"source":["if not os.path.exists(\"./models\"):\n","            os.makedirs(\"./models\")\n","for index, (train_index, val_index ) in enumerate(new_splits[0:], start=0):\n","    print(\"Fold : {}\".format(index))\n","\n","    batchsize = 16\n","    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, class_split=0.0)\n","    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n","\n","    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n","    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n","    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n","    test_preds_iter = np.zeros((2000000, 11))\n","    it = 0\n","    for it in range(1):\n","        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","        model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=64, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n","                         bidirectional=True).to(device)\n","\n","        no_of_epochs = 150\n","        early_stopping = EarlyStopping(patience=20, is_maximize=True, checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n","        criterion = L.FocalLoss()\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=0.001, epochs=no_of_epochs,\n","                                                steps_per_epoch=len(train_dataloader))\n","        avg_train_losses, avg_valid_losses = [], []\n","\n","\n","        for epoch in range(no_of_epochs):\n","            start_time = time.time()\n","\n","            print(\"Epoch : {}\".format(epoch))\n","            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n","            train_losses, valid_losses = [], []\n","\n","            model.train() # prep model for training\n","            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n","\n","            for x, y in train_dataloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                optimizer.zero_grad()\n","                predictions = model(x[:, :trainval.shape[1], :])\n","\n","                predictions_ = predictions.view(-1, predictions.shape[-1])\n","                y_ = y.view(-1)\n","\n","                loss = criterion(predictions_, y_)\n","                # backward pass: compute gradient of the loss with respect to model parameters\n","                loss.backward()\n","                # perform a single optimization step (parameter update)\n","                optimizer.step()\n","                schedular.step()\n","                # record training lossa\n","                train_losses.append(loss.item())\n","\n","                train_true = torch.cat([train_true, y_], 0)\n","                train_preds = torch.cat([train_preds, predictions_], 0)\n","\n","            model.eval() # prep model for evaluation\n","            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n","            with torch.no_grad():\n","                for x, y in valid_dataloader:\n","                    x = x.to(device)\n","                    y = y.to(device)\n","\n","                    predictions = model(x[:,:trainval.shape[1],:])\n","                    predictions_ = predictions.view(-1, predictions.shape[-1])\n","                    y_ = y.view(-1)\n","\n","                    loss = criterion(predictions_, y_)\n","                    valid_losses.append(loss.item())\n","\n","                    val_true = torch.cat([val_true, y_], 0)\n","                    val_preds = torch.cat([val_preds, predictions_], 0)\n","\n","            # calculate average loss over an epoch\n","            train_loss = np.average(train_losses)\n","            valid_loss = np.average(valid_losses)\n","            avg_train_losses.append(train_loss)\n","            avg_valid_losses.append(valid_loss)\n","\n","            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n","\n","            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n","\n","            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n","            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n","\n","            if early_stopping(val_score, model):\n","                print(\"Early Stopping...\")\n","                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n","                break\n","\n","            print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n","        with torch.no_grad():\n","            pred_list = []\n","            for x, y in test_dataloader:\n","                x = x.to(device)\n","                y = y.to(device)\n","\n","                predictions = model(x[:,:trainval.shape[1],:])\n","                predictions_ = predictions.view(-1, predictions.shape[-1])\n","\n","                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n","            test_preds = np.vstack(pred_list)\n","\n","        test_preds_iter += test_preds\n","        test_preds_all += test_preds\n","        if not os.path.exists(\"./predictions/test\"):\n","            os.makedirs(\"./predictions/test\")\n","        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n","        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)\n","\n","test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n","test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n","                                'open_channels': np.argmax(test_preds_all, axis=1)})\n","test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold : 0\n","Epoch : 0\n","learning_rate: 0.000001000\n","train_loss: 1.975269, valid_loss: 1.767695\n","train_f1: 0.048087, valid_f1: 0.053954\n","--- 39.26618957519531 seconds ---\n","Epoch : 1\n","learning_rate: 0.000011938\n","train_loss: 1.829952, valid_loss: 1.591058\n","train_f1: 0.053244, valid_f1: 0.053209\n","--- 39.3377902507782 seconds ---\n","Epoch : 2\n","learning_rate: 0.000044274\n","train_loss: 1.539125, valid_loss: 1.134693\n","train_f1: 0.071825, valid_f1: 0.069669\n","--- 39.119208335876465 seconds ---\n","Epoch : 3\n","learning_rate: 0.000096592\n","train_loss: 1.145123, valid_loss: 0.720945\n","train_f1: 0.111759, valid_f1: 0.075879\n","--- 38.71682119369507 seconds ---\n","Epoch : 4\n","learning_rate: 0.000166599\n","train_loss: 0.888250, valid_loss: 0.610245\n","train_f1: 0.163459, valid_f1: 0.173499\n","--- 38.69548439979553 seconds ---\n","Epoch : 5\n","learning_rate: 0.000251230\n","train_loss: 0.742371, valid_loss: 0.491547\n","train_f1: 0.225569, valid_f1: 0.290978\n","--- 38.50628185272217 seconds ---\n","Epoch : 6\n","learning_rate: 0.000346779\n","train_loss: 0.605946, valid_loss: 0.409659\n","train_f1: 0.301444, valid_f1: 0.376853\n","--- 38.313310861587524 seconds ---\n","Epoch : 7\n","learning_rate: 0.000449060\n","train_loss: 0.504674, valid_loss: 0.346806\n","train_f1: 0.366169, valid_f1: 0.455675\n","--- 37.735105752944946 seconds ---\n","Epoch : 8\n","learning_rate: 0.000553594\n","train_loss: 0.425397, valid_loss: 0.279148\n","train_f1: 0.428436, valid_f1: 0.542958\n","--- 37.09637999534607 seconds ---\n","Epoch : 9\n","learning_rate: 0.000655802\n","train_loss: 0.350974, valid_loss: 0.207278\n","train_f1: 0.500062, valid_f1: 0.712367\n","--- 37.34826302528381 seconds ---\n","Epoch : 10\n","learning_rate: 0.000751209\n","train_loss: 0.281635, valid_loss: 0.154017\n","train_f1: 0.581460, valid_f1: 0.761694\n","--- 36.93807911872864 seconds ---\n","Epoch : 11\n","learning_rate: 0.000835636\n","train_loss: 0.228190, valid_loss: 0.120796\n","train_f1: 0.647005, valid_f1: 0.801225\n","--- 36.95048379898071 seconds ---\n","Epoch : 12\n","learning_rate: 0.000905384\n","train_loss: 0.190879, valid_loss: 0.099720\n","train_f1: 0.699445, valid_f1: 0.821978\n","--- 36.397385597229004 seconds ---\n","Epoch : 13\n","learning_rate: 0.000957400\n","train_loss: 0.165360, valid_loss: 0.087850\n","train_f1: 0.734576, valid_f1: 0.829323\n","--- 36.43321204185486 seconds ---\n","Epoch : 14\n","learning_rate: 0.000989405\n","train_loss: 0.148844, valid_loss: 0.080186\n","train_f1: 0.760126, valid_f1: 0.836268\n","--- 36.45201635360718 seconds ---\n","Epoch : 15\n","learning_rate: 0.001000000\n","train_loss: 0.135699, valid_loss: 0.076720\n","train_f1: 0.780792, valid_f1: 0.837811\n","--- 36.43953847885132 seconds ---\n","Epoch : 16\n","learning_rate: 0.000999860\n","train_loss: 0.126957, valid_loss: 0.074178\n","train_f1: 0.793901, valid_f1: 0.840478\n","--- 36.33518195152283 seconds ---\n","Epoch : 17\n","learning_rate: 0.000999450\n","train_loss: 0.119988, valid_loss: 0.071525\n","train_f1: 0.803722, valid_f1: 0.842729\n","--- 36.28929805755615 seconds ---\n","Epoch : 18\n","learning_rate: 0.000998769\n","train_loss: 0.115916, valid_loss: 0.073325\n","train_f1: 0.810319, valid_f1: 0.837511\n","--- 36.163642168045044 seconds ---\n","Epoch : 19\n","learning_rate: 0.000997818\n","train_loss: 0.111414, valid_loss: 0.068915\n","train_f1: 0.817290, valid_f1: 0.843736\n","--- 36.301249742507935 seconds ---\n","Epoch : 20\n","learning_rate: 0.000996598\n","train_loss: 0.107288, valid_loss: 0.069829\n","train_f1: 0.823602, valid_f1: 0.841727\n","--- 36.87196397781372 seconds ---\n","Epoch : 21\n","learning_rate: 0.000995108\n","train_loss: 0.103672, valid_loss: 0.066571\n","train_f1: 0.829801, valid_f1: 0.845270\n","--- 36.7096381187439 seconds ---\n","Epoch : 22\n","learning_rate: 0.000993351\n","train_loss: 0.101263, valid_loss: 0.065524\n","train_f1: 0.837143, valid_f1: 0.845541\n","--- 36.92659521102905 seconds ---\n","Epoch : 23\n","learning_rate: 0.000991326\n","train_loss: 0.099135, valid_loss: 0.064655\n","train_f1: 0.842601, valid_f1: 0.848373\n","--- 36.944629192352295 seconds ---\n","Epoch : 24\n","learning_rate: 0.000989035\n","train_loss: 0.096957, valid_loss: 0.063764\n","train_f1: 0.848738, valid_f1: 0.856623\n","--- 36.94223928451538 seconds ---\n","Epoch : 25\n","learning_rate: 0.000986480\n","train_loss: 0.094961, valid_loss: 0.062429\n","train_f1: 0.854111, valid_f1: 0.888333\n","--- 36.666807889938354 seconds ---\n","Epoch : 26\n","learning_rate: 0.000983661\n","train_loss: 0.093373, valid_loss: 0.061885\n","train_f1: 0.857789, valid_f1: 0.905008\n","--- 36.73524308204651 seconds ---\n","Epoch : 27\n","learning_rate: 0.000980580\n","train_loss: 0.091370, valid_loss: 0.061056\n","train_f1: 0.862836, valid_f1: 0.922363\n","--- 36.2809042930603 seconds ---\n","Epoch : 28\n","learning_rate: 0.000977239\n","train_loss: 0.091153, valid_loss: 0.060553\n","train_f1: 0.868582, valid_f1: 0.926310\n","--- 36.09397101402283 seconds ---\n","Epoch : 29\n","learning_rate: 0.000973639\n","train_loss: 0.088831, valid_loss: 0.059603\n","train_f1: 0.871727, valid_f1: 0.928024\n","--- 36.123127460479736 seconds ---\n","Epoch : 30\n","learning_rate: 0.000969783\n","train_loss: 0.087525, valid_loss: 0.059025\n","train_f1: 0.876823, valid_f1: 0.930947\n","--- 36.51240873336792 seconds ---\n","Epoch : 31\n","learning_rate: 0.000965673\n","train_loss: 0.086349, valid_loss: 0.058208\n","train_f1: 0.879779, valid_f1: 0.933286\n","--- 36.48584461212158 seconds ---\n","Epoch : 32\n","learning_rate: 0.000961310\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wT0PFfEtYVS2"},"source":[],"execution_count":null,"outputs":[]}]}